---
title: Machine Learning Overview with Lending Club Dataset\thanks{I am very thankful to Robert McDonald and the Kellogg School of Management for the support and the help in writing this guide.}
author: "Enzo Profili"
date: "`r Sys.Date()`"
output: 
  bookdown::pdf_document2:
    template: null
    toc: true
    toc_depth: 2
    fig_caption: yes
    number_sections: true
header-includes:
   - \usepackage{float}
   - \floatplacement{figure}{H}
   - \usepackage{url}
   - \usepackage{array}
colorlinks: true
linkcolor: 'cyan'
geometry: "margin=1.25in"
fontsize: 12pt
bibliography: "MLoverview.bib"
csl: "apa.csl"
---

```{r setup, include=FALSE, warning = FALSE, message=FALSE}
# set working directory to data folder in dropbox
knitr::opts_knit$set(root.dir = getwd())
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, comment = NA, message = FALSE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=70),tidy=TRUE)
library("ggplot2")
library("tidyr")
library("dplyr")
library("scales")
library("randomForest")
library("caret")
library("DMwR")
library("pROC")
library("knitr")
library("data.table")
library("stringr")
library("kableExtra")
library("doSNOW")
library("corrplot")
library("nnet")
library("gridExtra")
```

\newpage

# Introduction

In Part II, we will dig deeper in some important concepts that can make the algorithm more accurate and customizable. We will now use the [Kaggle Lending Club dataset](https://www.kaggle.com/wendykan/lending-club-loan-data), which is much larger - more than 800,000 rows - and consequently computer speed will start to become a factor, so keeping the code as fast as possible is crucial. Of course, there are services such as Google Graphics Processing Unit and Amazon High Performance Computing that lend processing power.

# Data Cleaning

## Read in the data

Using the fread function (from the package data.table) is generally good practice, since it is generally faster than the read.csv function from base R.

```{r reading, message=FALSE, cache=TRUE}
library(tidyverse)
library(data.table)
loans <- fread('~/tex/dataexploration/data/loan.csv', 
               sep = ",",header = TRUE)
```

## Target Variable

The first step in any sucessful machine learning is understanding the data and adapting it for the model - this is called data cleaning.
In this dataset, our objective is to predict whether a loan will be paid in time. So, first of all, let's understand our target variable - loan status. There are `r length(table(loans$loan_status))` categories, as shown in Table \@ref(tab:status).

```{r statusfreq, results='markup', message=FALSE, echo=FALSE, cache=TRUE}
kable(as.data.frame(table(loans$loan_status)), 
      col.names = c('Category', 'Frequency'),
      caption = "Loan status categories in the Lending Club data.",
      booktabs = TRUE, 
      longtable = TRUE,
      label = "status") %>% kable_styling(latex_options=c("hold_position"))
```

To make it simpler, let's split the dataframe into good and bad loans; bad loans include defaulted and charged off, while good ones were fully paid; the other class (such as current loans or late for a short period of time) were ignored, since they do not provide enough insight on whether a loan is good or bad, and might reduce the signal from actually defaulted loans.

```{r loanstatus, results='asis', cache=TRUE}
# filter ambiguous status types 

loans <- filter(loans, loan_status != "Issued", 
                loan_status != "Current", 
                loan_status != "Does not meet the credit policy. Status:Charged Off",
                loan_status != "Does not meet the credit policy. Status:Fully Paid",
                loan_status != "In Grace Period",
                loan_status != "Late (16-30 days)",
                loan_status != "Late (31-120 days)")

# divide loan types into good and bad
loans <- mutate(loans, loan_quality = ifelse(loan_status == "Fully Paid", 
                                             "Good", "Bad"))  
loans$loan_quality <- as.factor(loans$loan_quality)
```

## Predictor Variables

After, we need to check for the predictor variables. We have to: 

* delete data that is impractical to work with (such as ones with long explanations by the borrower)
* delete data that spills information from the future (since the model will not have future information to predict loans)
* delete variables with too many missing values
* delete data with insufficient variation (only one value, for example)
* see if there are variables that require manipulation or filtering to meet our needs

Some new variables were created out of the initial ones. For example, the variable year, which states the year the loan was taken (and might be useful since macroeconomic conditions vary), and the amount of years between the loan was taken and the earliest credit line for the client was created (older accounts may be less risky). Some new variables were taken from @polena:5.^[See @polena:5 chapter 5 for more details.] The list of variables that remained are in the [remained variables definition section](#remaindef), Section \@ref(remaindef). This section also contains the description of all created variables.

```{r addvars, results='asis', cache=TRUE}
# new variable: income to amount relationship
loans <- mutate(loans, 
                loan_income_ratio = loans$annual_inc/loans$loan_amnt)

# new variable: collection in last 12 months
loans <- mutate(loans, 
                collections = ifelse(collections_12_mths_ex_med > 0, 1, 0))  
loans$collections <- as.factor(loans$collections)

# new variable: last credit pulled
loans <- mutate(loans, 
                last_credit_pull_d_v2 = str_sub(last_credit_pull_d,-4,-1))
loans$last_credit_pull_d_v2 <- as.factor(loans$last_credit_pull_d_v2)

# new variable: loan year
loans <- mutate(loans, year = str_sub(issue_d,-4,-1))
loans$year <- as.numeric(loans$year)

# new variable: credit line age
loans <- mutate(loans, 
                credit_line_age = as.numeric(year) 
                - as.numeric(str_sub(earliest_cr_line,-4,-1)))
loans$year <- as.factor(loans$year
                        )
# new variable: description length
loans <- mutate(loans, desc_length = nchar(desc))
loans$desc_length <- as.numeric(loans$desc_length)
```

Another variable of potential interest is `desc`.  This field contains explanations about the borrowing provided by the borrower. Obviously this will not be standardized and it will be difficult to interpret, but it might be interesting to see if the length of the explanation has any explanatory power. Thus, we create `desc_length`, which is the number of characters in the field.

Finally, we remained with 26 variables out of the initial 74. Many variables were removed because they report future characteristics of the loan, or are unique to each loan. Descriptions of all removed variables are in the [removed variables definition section](#removeddef), Section \@ref(removeddef).

```{r removevars, results='asis', cache=TRUE}
# filter irrelevant/future features
drop_cols <- c('desc','url', 'id', 'member_id', 'funded_amnt', 
               'funded_amnt_inv', 'grade', 'int_rate', 'emp_title', 
               'zip_code', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 
               'total_pymnt_inv', 'total_rec_prncp','total_rec_int',
               'total_rec_late_fee', 'recoveries', 
               'collection_recovery_fee', 'last_pymnt_d', 
               'last_pymnt_amnt', 'title', 'verification_status_joint', 
               'next_pymnt_d', 'issue_d', 'loan_status', 
               'last_credit_pull_d', 'earliest_cr_line', 'pymnt_plan', 
               'application_type', 'policy code', 
               'collections_12_mths_ex_med', 'policy_code', 
               'tot_coll_amt', 'collections_12_mths_ex_med', 
               'collections')
loans <- loans %>% dplyr::select(-one_of(drop_cols))
```

## Missing Values

Delete variables that are predominantly missing; see Table \@ref(tab:missing).

```{r missing, results='asis', cache=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff = 40)}
library(purrr)
missing <- map_dbl(loans, ~ sum(is.na(.))/length(.))
loans <- loans[ missing < 0.5]
kable(missing[missing > 0.5],
      caption = 'Variables that have been deleted for numerous missing values',
      col.names = c('Fraction missing'),
      label = 'missing',
      longtable = TRUE,
      booktabs = TRUE) %>% kable_styling(latex_options=c("hold_position"))
```

## Insufficient Variation

We can use the Herfindahl index to measure the variation contained in a variable. It is unclear what to use as a cutoff. Setting it equal to 0.95 eliminates one variable, acc_now_delinq; see Table \@ref(tab:herf).

```{r variation, results='asis', cache=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff = 40)}
cutoff_herf <- 0.95
herfindahl <- map_dbl(loans, ~sum(prop.table(table(.))^2))
loans <- loans[herfindahl < cutoff_herf]
kable(herfindahl[herfindahl >= cutoff_herf], 
      booktabs = TRUE, 
      col.names = 'Herfindahl index', 
      caption = paste0('Herfindahl indices exceeding ', 
                       cutoff_herf),
      label = 'herf') %>% kable_styling(latex_options=c("hold_position"))

# remove factor levels with one observation (to avoid errors in the logistic
# regression down the road)
loans <- loans[loans$addr_state != "IA", ]
```

## Collinearity/Scaling

Another assesment that should be made before proceeding is predictor collinearity. If two variables are highly correlated, then having both is redundant and will only make the model more unstable and computationally expensive. Therefore, it is advised to remove one of the correlated variables. Let's check the correlation matrix for our predictors:

```{r corr, cache=TRUE, fig.cap= "Predictor correlation plot"}
corrplot(cor(loans[complete.cases(loans), sapply(loans, is.numeric)]))
loans <- loans %>% dplyr::select(-loan_amnt)
```

Notice there is very large correlation between the predictors loan_amnt and installment, so let's remove loan_amnt from the dataset (as it is slightly more correlated with other predictors). Apart from that, we are good to go.

One last point to be made is about data scaling. Since we have numeric predictors with many different values (e.g. income ranges from 0 to 300,000 while age ranges from 0 to 50), the ones with larger values (such as income) may influence more the final result. This happens especially in neural networks, but not in random forest (or logistic regression), as it only needs the absolute value to split nodes (little data preparation is an advantage of random forests). One drawback of normalization is that regression coefficients no longer make sense. The default for the scale function in R is the following:
$$x' = \frac{x-\bar{x}}{\sigma}$$
$\sigma$ represents the column standard deviation, and $\bar{x}$ is the column mean. We will be running neural networks in this report, so I have scaled the data with the scale function, following the procedure in @beque:1. We will compare results between normalized and unnormalized data for random forests and neural networks to show the impact of normalization. ^[For more data pre-processing techniques analysis, I recommend checking @crone:1.]

```{r scale, cache=TRUE}
# we need to add function(x) description so that scale does not return 
# attributes, and type remains unchanged
scale.loans <- loans %>% mutate_if(is.numeric,  
                                   function(x) as.numeric(scale(x)))
```

## The end result

This leaves us with `r ncol(loans)` variables and `r nrow(loans)` observations; see Table \@ref(tab:finalvars):

```{r endresult, cache=TRUE, echo = FALSE}
endresult <- data.frame('Variable' = names(loans),
                 'Missing' = sapply(loans, function(a) sum(is.na(a)))) 
kable(endresult, row.names = FALSE, 
      caption = 'Variables in the final data set', 
      longtable = TRUE,
      booktabs = TRUE, 
      label = 'finalvars') %>% kable_styling(latex_options=c("hold_position"))
```

# Data Analysis 

Now that we have adjusted our data, let's analyze it. We find many factors that may be important to predict bad loans, such as credit score, loan purpose, location and employment stability. So let's see if that's the case!

```{r barplot, results='asis', cache=TRUE, fig.height = 4, fig.width = 12, echo = FALSE, fig.cap= 'Default rates for credit subgrades'}
# loan quality by sub-grade
ggplot(loans, aes(x = sub_grade, fill = loan_quality))+
  geom_bar(position = "fill")+
  scale_y_continuous(labels = percent_format())+
  labs(fill="Loan Quality")+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black"),
        rect = element_rect(fill = "transparent"), axis.text.x=element_text(angle = 45),
        text = element_text(size=16), axis.title.y= element_blank())
```

```{r barplot2, results='asis', cache=TRUE, fig.height = 4, fig.width = 12, echo = FALSE, fig.cap= 'Default rates by year loan started'}
# loan quality by year
ggplot(loans, aes(x = year, fill = loan_quality))+
  geom_bar(position = "fill")+
  scale_y_continuous(labels = percent_format())+
  labs(fill="Loan Quality")+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black"),
        rect = element_rect(fill = "transparent"), axis.text.x=element_text(vjust = 0.5),
        text = element_text(size=16), axis.title.y= element_blank())
```

```{r barplot3, results='asis', cache=TRUE, fig.height = 4, fig.width = 12, echo = FALSE, fig.cap= 'Default rates by credit line age', out.extra='trim={0 0.25cm 0 0.3cm},clip'}
# loan quality by credit line age
ggplot(loans, aes(x = credit_line_age, fill = loan_quality))+
  geom_bar(position = "fill")+
  scale_y_continuous(labels = percent_format())+
  labs(fill="Loan Quality")+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black"),
        rect = element_rect(fill = "transparent"),
        text = element_text(size=16), axis.title.y= element_blank())
```

```{r barplot4, results='asis', cache=TRUE, fig.height = 5, fig.width = 12, echo = FALSE, fig.cap= 'Default rates by purpose', out.extra='trim={0 0.25cm 0 0.3cm},clip'}
# loan quality by loan purpose
ggplot(loans, aes(x = purpose, fill = loan_quality))+
  geom_bar(position = "fill")+
  scale_y_continuous(labels = percent_format())+
  labs(fill="Loan Quality")+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black"),
        rect = element_rect(fill = "transparent"), axis.text.x=element_text(angle = 90, vjust = 0.5, hjust = 1),
        text = element_text(size=16), axis.title.y= element_blank())
```

```{r barplot5, results='asis', cache=TRUE, fig.height = 4, fig.width = 12, echo = FALSE, fig.cap= 'Default rates by employment length', out.extra='trim={0 0.25cm 0 0.3cm},clip'}
# loan quality by employment length
ggplot(loans, aes(x = emp_length, fill = loan_quality))+
  geom_bar(position = "fill")+
  scale_y_continuous(labels = percent_format())+
  labs(fill="Loan Quality")+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black"),
        rect = element_rect(fill = "transparent"), axis.text.x=element_text(angle = 90, vjust = 0.5, hjust = 1),
        text = element_text(size=16), axis.title.y= element_blank())
```

```{r barplot6, results='asis', cache=TRUE, fig.height = 4, fig.width = 12, echo = FALSE, fig.cap= 'Default rates by home ownership'}
# loan quality by home ownership
ggplot(loans, aes(x = home_ownership, fill = loan_quality))+
  geom_bar(position = "fill")+
  scale_y_continuous(labels = percent_format())+
  labs(fill="Loan Quality")+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black"),
        rect = element_rect(fill = "transparent"),
        text = element_text(size=16), axis.title.y= element_blank())
```

```{r barplot7, results='asis', cache=TRUE, fig.height = 4, fig.width = 12, echo = FALSE, fig.cap= 'Default rates by state'}
# loan quality by state
ggplot(loans, aes(x = addr_state, fill = loan_quality))+
  geom_bar(position = "fill")+
  scale_y_continuous(labels = percent_format())+
  labs(fill="Loan Quality")+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black"),
        rect = element_rect(fill = "transparent"), axis.text.x=element_text(angle = 90),
        text = element_text(size=16), axis.title.y= element_blank())
```

From these graphs, we see that credit grade and credit line age show a meaningful relationship with default rates. Not surprisingly, higher credit grades determine less bad loans. The other variables might possibly be helpful, but the pattern is less clear. Now, let's analyze continuous variables, such as income, with density plots:

```{r densityplots, results='asis', echo = FALSE, cache=TRUE, fig.cap='Density plots of default rates against various continuous predictors'}
# loan quality by annual income
plot_1 <- ggplot(loans, aes(x = annual_inc, fill = loan_quality)) +
  geom_density(alpha = 0.5) +
  coord_cartesian(xlim = c(0, 200000))+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black"),
        rect = element_rect(fill = "transparent"), axis.text.x=element_text(angle = 0),
        text = element_text(size=10), legend.position = "none")

# loan quality by credit revolving balance
plot_2 <- ggplot(loans, aes(x = revol_bal, fill = loan_quality))+
  geom_density(alpha= 0.5)+
  coord_cartesian(xlim = c(0, 75000))+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black"),
        rect = element_rect(fill = "transparent"), axis.text.x=element_text(angle = 0),
        text = element_text(size=10), legend.position = "none")

# loan quality by how much of revolving credit was used
plot_3 <- ggplot(loans, aes(x = revol_util, fill = loan_quality))+
  geom_density(alpha = 0.5)+
  coord_cartesian(xlim = c(0, 100))+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black"),
        rect = element_rect(fill = "transparent"),
        text = element_text(size=10), legend.position = "none")

# loan quality by loan amount
plot_4 <- ggplot(loans, aes(x = installment, fill = loan_quality))+
  geom_density(alpha = 0.5)+
  coord_cartesian(xlim = c(0, 1500))+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black"),
        rect = element_rect(fill = "transparent"),
        text = element_text(size=10), legend.position = "none")

# loan quality by annual income/loan amount
plot_5 <- ggplot(loans, aes(x = loan_income_ratio, fill = loan_quality))+
  geom_density(alpha = 0.5)+
  coord_cartesian(xlim = c(0, 20))+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black"),
        rect = element_rect(fill = "transparent"),
        text = element_text(size=10), legend.position = "none")

grid.arrange(plot_1, plot_2, plot_3, plot_4, plot_5, nrow=3, ncol=2)
```

These plots show us some interesting data features. We see that income and loan-income ratio show considerable difference between good and bad loans. The percentage of revolving credit used and loan amount too. So, these variables might be relevant to address defaults. These are good indicators that the data is somewhat useful to explain defaulting.

# Data Partitioning

So, let's generate models with all variables deemed relevant. Only a subset of the data was used, since it makes calculations faster and does not interfere significantly on the model's accuracy. We then split the data between training and testing datasets.

```{r datasplit, results='markup', cache=TRUE}
# take only 200000 rows (saves processing power)
set.seed(1981)
indexes <- sample(nrow(loans), 200000)
loans_subset <- loans[indexes,]
loans_subset <- as.data.frame(unclass(loans_subset))
scale.loans_subset <- scale.loans[indexes,]
scale.loans_subset <- as.data.frame(unclass(scale.loans_subset))

# split the data into two similar sets, one for training and one for 
# testing (explained later), keeping similar default ratio
set.seed(2437)
splitIndex <- createDataPartition(loans_subset$loan_quality, p = .70,
                                  list = FALSE,
                                  times = 1)
trainSplit <- loans_subset[ splitIndex,]
testSplit <- loans_subset[-splitIndex,]
scale.trainSplit <- scale.loans_subset[ splitIndex,]
scale.testSplit <- scale.loans_subset[-splitIndex,]

# filter for complete cases in both training and test datasets
trainSplit <- trainSplit %>% filter(complete.cases(trainSplit))
testSplit <- testSplit %>% filter(complete.cases(testSplit))

scale.trainSplit <- scale.trainSplit %>% filter(complete.cases(scale.trainSplit))
scale.testSplit <- scale.testSplit %>% 
filter(complete.cases(scale.testSplit))

# create matrix of predictors
rf_predictors <- trainSplit[, !names(trainSplit) 
                              %in% c("loan_quality")]
rf_predictors_test <- testSplit[, !names(testSplit) 
                                  %in% c("loan_quality")]

scale.rf_predictors <- scale.trainSplit[,!names(scale.trainSplit) 
                                         %in% c("loan_quality")]
scale.rf_predictors_test <- scale.testSplit[, !names(scale.testSplit) 
                                              %in% c("loan_quality")]
```

When reducing the dataset, you will notice the data was split into two datasets: trainSplit and testSplit. The names are self-explanatory, but trainSplit is used to train the model, while testSplit is a dataset the model has never seen, and so the model can be accurately tested. This is called the validation set approach. The datasets share a constant bad loan proportion to ensure reproducibility (this is the default setting in caret's createDataPartition function).
  
Random forests already conduct this testing process, as each tree uses only around two thirds of the data, and the other third is used for testing. The OOB error rate is an useful estimate of the test error rate, but, whenever possible, it is good to use the test dataset as well. ^[check @james:1 pages 317-319 for more details and an example.]

# Logistic Regression

As we are trying to predict a binary variable, logistic regression is a viable option to estimate the probability of default. Since we are discussing probabilities, the results must be between 0 and 1. However, note that a linear regression in this case could yield probabilities above 1 or below 0 for some predictor values.^[@james:1 displays a graphical example of this issue in page 131.] So, logit adapts the linear regression by using the exponential of the predictors. Check the following equation:

$$Pr(Y_{i}=1|X_{i}) = \frac{e^{X_{i}\beta}}{1+e^{X_{i}\beta}}$$

It becomes clear that, as $X_{i}\beta\to\infty$, $Pr(Y_{i}=1|X_{i})\to$ 1, and as $X_{i}\beta\to-\infty$, $Pr(Y_{i}=1|X_{i})\to$ 0. So, $Pr(Y_{i}=1|X_{i})\in$ [0,1]. In our bank loans example, each $Y_{i}=1$ represents a loan that can be assigned either as "Good" or "Bad". The logit model will compute the probability the loan is bad and plot the logit transformation.^[See @james:1 pages 130-137 for a more detailed explanation on logistic regressions.]

```{r glm, results='markup', cache=TRUE}
#run logistic regression
logistic = glm(loan_quality ~ ., data = trainSplit, family = binomial(logit))

# predict loan quality for test dataset and generate confusion matrix
glm.probs = predict(logistic, testSplit, type = "response")
glm.pred = rep("Bad", nrow(testSplit))
glm.pred[glm.probs >.5] = "Good"
kable(table(glm.pred , testSplit$loan_quality))
mean(glm.pred == testSplit$loan_quality)
```

The model generates a dummy variable for every factor variable (except one for each predictor to avoid multicollinearity), and consequently we have hundreds of predictors. A large portion of them are in fact significant at the 0.1% level, which shows the quality of our manipulated data. We see that the logistic regression has an accuracy rate of 81% on the test dataset. Let's now analyze nonparametric models on this dataset and compare.

# Random Forests

## Regular Random Forest

Let's run our random forest. The default value for mtry is $\sqrt{p}$, where p is the number of predictors. Since there are 25 predictors in the dataset, mtry = 5. Of course, we could change this value, but let's leave it for now.

```{r rf1, results='markup', cache=TRUE}
# run random forest
set.seed(1234)
rf1 <- randomForest(x = rf_predictors, 
                    y = trainSplit$loan_quality, 
                    importance = TRUE,
                    ntree = 300)
rf1

# run rf with scaled predictors
set.seed(1234)
scale.rf1 <- randomForest(x = scale.rf_predictors, 
                    y = scale.trainSplit$loan_quality, 
                    importance = TRUE,
                    ntree = 300)
scale.rf1
```

Notice there was almost no difference between the random forests with scaled and non-scaled data, with a 81% OOB accuracy (similar to the logistic regression).

## SMOTE 

The random forest test had a relatively small OOB error (19%), but, since bad loans are less frequent (25%), it almost never predicted it, and so the model adds almost no value to the analysis. A possible solution to this problem is over-sampling bad loans and under-sampling good loans in the dataset- this method is called SMOTE (SMOTE: Synthetic Minority Over-sampling Technique). This way, we will have more bad loans and it will be easier to predict them. 

In R, the function SMOTE (Synthetic Minority Over-sampling Technique) from the package DMwR does the trick.^[For the interested reader, @chawla:1 explains SMOTE more thoroughly.] With this function, you set how much the rare event is oversampled (through the perc.over argument) and the common event is undersampled (through the perc.under argument). In @weiss:1, it was shown that the optimal class distribution should contain between 50% and 90% minority class examples within the training set. So, let's stick to this measure (50%, more specifically). Note as well you should only apply SMOTE to the training data, as otherwise you will be generating samples that are closely related to the training set and you will get an overly optimistic accuracy rate.

Notice that, the larger the proportion minority class samples in the training set, the more it will be predicted in the testing dataset. Thus, if classifying a bad loan as good as more costly than the opposite, you should have a training set with many defaulted loans (relative to good ones). See below a random forest applied to a balanced dataset:

```{r SMOTE, results='markup', cache=TRUE}
# balance data with SMOTE
kable(table(trainSplit$loan_quality))
balanced.data <- SMOTE(loan_quality ~., trainSplit, 
                       perc.over = 200, perc.under = 150)
kable(table(balanced.data$loan_quality))
rf_predictors_balanced <- balanced.data[, !names(balanced.data) 
                                        %in% c("loan_quality")]
```


```{r rf2, results='markup', cache=TRUE}
# run random forest and plot results
set.seed(1234)
rf2 <- randomForest(x = rf_predictors_balanced, 
                    y = balanced.data$loan_quality, 
                    importance = TRUE,
                    ntree = 300)
rf2
```

Through this technique, we could increase accuracy in predicting bad loans from 2% to 77%, and the OOB error rate actually decreased (though results may vary). Finally, just an explanation on the math in perc.over and perc.under (since it can be confusing). Originally, there were approximately 20000 bad loans. The number of bad loans added is Perc.over/100. So perc.over = 200% triples the original, and so we have 60000 examples. Perc.under adds 200% of new bad loans in perc.over, so that the amount of good loans are twice the amount of bad loans added. So, the number of good loans are 200*200/100 = 400% of the original number of bad loan cases (80000).  

## Cutoffs

Another way to predict more bad loans runs through an important concept: cutoffs. They are the proportion at which a vote is decided. By default, random forests have a cutoff of .5, which means that if more than 50% of trees decide a loan is bad, it returns "Bad Loan". However, if in your model you would like to catch more bad loans (at the cost of cathing additional good loans unintentionally), you can set a lower cutoff (say, 0
0.2). This way, if 20% or more trees vote for "Bad Loan", the model returns "Bad Loan", even if 80% of the trees voted against it. Below is a random forest with cutoff set to .3:

```{r rf3, results='markup', cache=TRUE, tidy=TRUE, tidy.opts=list(width.cutoff = 40)}
set.seed(1234)
rf3 <- randomForest(x = rf_predictors, 
                    y = trainSplit$loan_quality, 
                    importance = TRUE,
                    cutoff = c(0.2, 0.8),
                    ntree = 300)
rf3
```

It can be seen above that, by applying a .2 cutoff, more bad loans were predicted (63%), but 30% of good loans were also deemed bad, culminating in a higher OOB error rate (31%). Since most bad loans were still not caught, a stricter cutoff could be implemented. So, this can be a very important tool if used carefully, and can be used to tailor the random forest algorithm to a certain business' needs.

# ROC Curve

## Explanation

But what if we knew the proportions of bad loans caught for every cutoff level? The ROC (Receiver Operating Characteristic) curve is the answer - a way to assess the performance of the model. Given the first random forest trained (rf1), we will try to predict bad loans with new values (from the dataset testSplit), and see how well the model performs with these curves. First, some concepts need to be clarified:
  
a) Sensitivity/recall: the ratio of true positives (positives that the model identified) and the total number of positives (true positives + false negatives). Basically, the ratio of positive results that were correctly predicted. From here on, let`s consider this a test for good loans, which means good loans should return "positive" and bad loans should return "negative".
  
b) Specificity: the ratio of true negatives (negatives that the model identified) and the total number of negatives (true negatives + false positives). Basically, the ratio of negative results that were correctly predicted.

The table below shows what is in each category in a confusion matrix. The row values are the actual ones (so that actual negatives is TN + FP), and the column show predicted values:

```{r table, results='markup', echo=FALSE, cache=TRUE}
cm <- matrix(c("TN", "FP", "FN", "TP"), ncol = 2, byrow = TRUE)
colnames(cm) <- c("0", "1")
rownames(cm) <- c("0", "1")
kable_styling(kable(cm), 
              bootstrap_options = c("striped", "hover", "responsive"))
```
  
Clearly, there is a tradeoff between sensitivity and specificity. In our example, we can simply assume that every loan is good (supposing a "positive" value is a good loan), and thus have zero specificity and one sensivity. In the other hand, we can assume every loan is bad, and have one specificity and zero sensivity. Probably, the ideal result is between one of these two scenarios, and that is what the ROC curve shows.
  
As can be seen in Figure \@ref(fig:ROCrf1), the y axis is the sensivity and the x axis is 1 - specificity. There is also a line between (0,1) and (1,0), showing the performance of a model that picks results randomly. The triangle below this line has an area under the curve (AUC) of 0.5. A machine learning model's AUC should be as close to 1 as possible, achieving sensivity close to 1 without giving up specificity. Graphically, this would mean the curve "hugs" the top left corner. 

## Random Forest Comparison

Given this explanation, let's see how rf1, rf2 and rf3 fared. Since we want to focus on the number of bad loans caught, we have to focus on the specificity measure.

```{r smotetest, cache = TRUE}
rf1.pred = predict(rf1, testSplit, type = "class")
rf1_table = table(testSplit$loan_quality,rf1.pred)
mean1 = mean(rf1.pred == testSplit$loan_quality)
rf2.pred = predict(rf2, testSplit, type = "class")
rf2_table = table(testSplit$loan_quality,rf2.pred)
mean2 = mean(rf2.pred == testSplit$loan_quality)
rf3.pred = predict(rf3, testSplit, type = "class")
rf3_table = table(testSplit$loan_quality,rf3.pred)
mean3 = mean(rf3.pred == testSplit$loan_quality)
oobs <- c((rf1$confusion[1,1]+rf1$confusion[2,2])/nrow(trainSplit),
          (rf2$confusion[1,1]+rf2$confusion[2,2])/nrow(balanced.data),
          (rf3$confusion[1,1]+rf3$confusion[2,2])/nrow(trainSplit))
rfs <- c("rf1","rf2","rf3")
means <- c(mean1,mean2,mean3)
spec <- c(rf1_table[1,1]/(rf1_table[1,1]+rf1_table[1,2]),
          rf2_table[1,1]/(rf2_table[1,1]+rf2_table[1,2]),
          rf3_table[1,1]/(rf3_table[1,1]+rf3_table[1,2]))
sens <- c(rf1_table[2,2]/(rf1_table[2,1]+rf1_table[2,2]),
          rf2_table[2,2]/(rf2_table[2,1]+rf2_table[2,2]),
          rf3_table[2,2]/(rf3_table[2,1]+rf3_table[2,2]))
rf_results = data.frame(rfs,oobs,means,sens,spec)
  colnames(rf_results) = c("RF", "OOB", "Testing Accuracy", 
                           "Sensivity", "Specificity")
kable(rf_results) %>% row_spec(0,bold=TRUE)
```

## Random Forest ROC

Let's see how our initial model (rf1) fares in this analysis. Below, we see that it has an AUC of .73, an interesting start. Bear in mind this is not a direct accuracy measure. The accuracy of the model on the testing dataset was 81%, not 73%. The AUC for logistic regression we ran was .72, slightly lower than the random forest. The AUC for rf2 (the model with SMOTE) is 0.71, a bit lower than the other two.

```{r ROCrf1, results='markup', cache=TRUE, fig.cap="ROC Curve for regular random forest"}
# get auc value
roc_predict <- predict(rf1, newdata = testSplit, na.rm = TRUE, type = 'prob')
auc <- roc(testSplit$loan_quality, roc_predict[,2])

# plot roc curve
plot(auc,main="ROC Curve for Random Forest",col=2,lwd=2)
print(auc)
```

```{r ROCSmote, results='markup', cache=TRUE, fig.cap="ROC Curve for random forest with SMOTE"}
# get auc value
roc_predict_smote <- predict(rf2, testSplit, 
                             na.rm = TRUE, type = 'prob')
auc_smote <- roc(testSplit$loan_quality, roc_predict_smote[,2])

# plot roc curve
plot(auc_smote,main="ROC Curve for Random Forest with SMOTE",col=2,lwd=2)
print(auc_smote)
```

```{r ROClogit, results='markup', cache=TRUE, fig.cap="ROC Curve for logistic regression"}
# get auc value for logistic regression
auc_logit <- roc(testSplit$loan_quality, glm.probs)

# plot roc curve
plot(auc_logit,main="ROC Curve for Logistic Regression",col=2,lwd=2)
print(auc_logit)
```

## Finding the Best Cutoff

Now that we have the ROC curves, we can make an informed decision as to which cutoff we might want to use. A method available in the pROC library is finding the cutoff that is closest to the top left corner. We will run rf1's ROC again, with only a subset of the test set (which we will call evaluating set), so that this set is independent from the test set. Then we will run prediction with the best cutoff using the rest of the test subset. See below in Table \@ref(tab:bestcutoff) bad loans are predicted approximately 70% of the time, while 35% of good loans are considered bad - an improvement over the original models.

```{r bestcutoff, results='markup', cache=TRUE, fig.cap="ROC Curve for regular random forest"}
# create evaluating test set and rest
set.seed(1341)
indexes <- sample(nrow(testSplit), 10000)
evaluating_set <- testSplit[indexes,]
test_set_subset <- testSplit[-indexes,]

# run rf1 ROC on evaluating set
roc_predict <- predict(rf1, newdata = evaluating_set, na.rm = TRUE, type = 'prob')
auc <- roc(evaluating_set$loan_quality, roc_predict[,2])

# get "best" cutoff value
rf1_threshold <- coords(auc, x = "best", best.method = "closest.topleft")
rf1_threshold

# predict test_set_subset results with cutoff value
subset_predict <- predict(rf1, newdata = test_set_subset, na.rm = TRUE, type = 'prob')
best_cutoff_predictions <- factor(ifelse(subset_predict[,2] > rf1_threshold[1], "Good", "Bad"))

kable(table(test_set_subset$loan_quality, best_cutoff_predictions),
      caption = 'Confusion Matrix using "best" cutoff threshold',
      label = 'bestcutoff') %>% kable_styling(latex_options=c("hold_position"))
```

# K-fold Cross Validation

Another important topic when tuning machine learning models is k-fold cross validation. Notice that when we created training and testing datasets, we are only fitting the training data to generate the model. This means we are losing the information in the testing dataset when it is separated. K-fold cross validation deals with this problem. This cross validation method divides the data sets into k splits, and tests it k times - every split is held as the testing dataset once. Then, it averages the error rate to arrive at an accuracy figure. This process can be repeated as many times as requested (in the code below it was repeated twice) to increase reliability in predicting accuracy. To ensure the model is trained correctly, it is important that in each split the proportion of good and bad loans is the same, otherwise there might be a chunk in which there are no defaulted loans. This is called stratified cross validation.^[@youtube is a very good introduction to cross validation.]

One additional feature in this method is that it allows for the optimization of hyperparameters (such as mtry, ntree, etc.), by running the algorithm for a specified amount of times (in the code below, 4 times for mtry). Bear in mind that rerunning the code so much may cause it to be very slow, and take hours (sometimes days!) to run. That's why I used the doSNOW package, which specifies how many threads should be used while running the code. Since my computer has 8 threads, I set 6 threads for this task, leaving some space for other applications (if necessary). See the code below for results:

```{r kfold, results='markup', cache=TRUE, fig.cap="AUC for different mtry values"}
# Create 15 total folds to maintain default ratio.
set.seed(2346)
cv.10.folds <- createMultiFolds(trainSplit$loan_quality, k = 4, times = 2)

# Set up caret's trainControl object
# 4- fold validation repeated 2x, parameter search specified (not random)
ctrl.1 <- trainControl(method = "repeatedcv", number = 4, 
                       repeats = 2, index = cv.10.folds,
                       classProbs = TRUE, 
                       summaryFunction = twoClassSummary,
                       search = "grid")


# doSNOW package on multi-core training, since we're going
# to be training a lot of trees.
cl <- makeCluster(6, type = "SOCK")
registerDoSNOW(cl)


# Set seed for reproducibility and train (ROC as performance measure)
# Testing lower mtry values since splits with all variables make no sense
set.seed(1234)
rf4.cv1 <- train(x = rf_predictors, y = trainSplit$loan_quality, 
                 method = "rf", tuneGrid = expand.grid(mtry = c(2, 4, 6, 8)),
                 ntree = 300, trControl = ctrl.1, 
                 metric = "ROC")

# Shutdown cluster
stopCluster(cl)
remove(cl)
registerDoSEQ()

# Check out results
rf4.cv1
plot(rf4.cv1)
```

```{r kfoldroc, results='markup', cache=TRUE, fig.cap="ROC Curve for 4-fold Cross Validation"}
# roc/auc in test data
bad_loan.rf.pr_cv <- predict(rf4.cv1, testSplit, na.rm = TRUE, type = 'prob')
auc3 <- roc(testSplit$loan_quality, bad_loan.rf.pr_cv[,2])

# Plot roc curve
plot(auc3,main="ROC Curve with K-Fold Cross Validation",col=2,lwd=2)
print(auc3)
```

With testing, we see that the best AUC results occur with mtry = 6, when compared to 2, 6 and 8. The difference is subtle- at most 0.03. However, the values for sensivity are very low, which means the model is assuming almost everything to be a good loan, and ignoring the bad ones. In this scenario, we achieve an AUC of .73 with the test data, which is only slightly better than our earlier test.

# Neural Networks

## Overview

Neural networks are another nonlinear model that can be applied in machine learning. The basic structure of the model follows like this:

1) Linear combinations of the predictors are taken to form what is called hidden units. In Figure \@ref(fig:nnetsgraph), the predictors (the inputs) are I1 and I2, while the hidden units are H1 and H2.

2) Each hidden unit goes through a transformation (usually logistic) so that the results are between 0 and 1.

3) Finally, a linear combination of the hidden values are taken to arrive at the outcome (in case of a regression), or a value $f_{il}(x)$ (for classification), which means it is the predicted probability of the outcome, as a function of x (the predictors) in the $i^{th}$ sample and the $l^{th}$ class. In Figure \@ref(fig:nnetsgraph), the outcome is O1.

The model above would have one layer of hidden units (data flows in the order input$\to$hidden unit$\to$output), which, in most cases, is enough. The other possibility would be input$\to$hidden unit$\to$hidden unit$\to$output, for a two-layer neural network, but that is not necessary in our case. Figure \@ref(fig:nnetsgraph) shows a neural network with 2 inputs and 2 neurons in the hidden layer (B1 and B2 stand for the intercepts in each layer):

```{r nnetsgraph, results='hide', cache=TRUE, echo=FALSE, fig.height = 3.5, fig.width = 5, fig.cap="Neural network graphical example"}
library(devtools)
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')
mod.in<-c(13.12,1.49,0.16,-0.11,-0.19,-0.16,0.56,-0.52,0.81)
struct<-c(2,2,1) #two inputs, two hidden, one output 
plot.nnet(mod.in,struct=struct)
dev.off()
```

Below, we are running a simple neural network, with only 3 neurons in the hidden layer. We then predict which loans will default on the test dataset, using this model.

```{r nnet1, results='markup', cache=TRUE}
# 1st neural net: unscaled data, only 3 neurons
set.seed(1492)
nn1 <- nnet(loan_quality ~ ., 
            data = trainSplit, 
            size = 3, 
            trace = FALSE)

# show table with results
kable(table(testSplit$loan_quality, 
            predict(nn1, testSplit, type = "class")))

# predict results on test data and provide AUC
bad_loan.nn1.pr <- predict(nn1, testSplit, na.rm = TRUE, type = 'raw')
auc_nn1 <- roc(testSplit$loan_quality, bad_loan.nn1.pr)
print(auc_nn1$auc)
```

## Scaling

Notice in nn1 how the network predicts all loans to be good, and that the ROC is close 0.5 (a random predictor). Thus, this model adds almost no value to the analysis. This happened because the data was not normalized - data normalization is especially important for neural networks. So, for nn2, I scaled all numeric predictors to have mean 0 and standard deviation 1. This transformation makes the neural network much more effective in predicting defaults (ROC increased from 0.53 to 0.73).

```{r nnet2, results='markup', cache=TRUE}
# 2th neural net: scaled data
set.seed(1492)
nn2 <- nnet(loan_quality ~ ., 
            data = scale.trainSplit, 
            size = 3, 
            trace = FALSE)

# show table
kable(table(scale.testSplit$loan_quality, 
            predict(nn2, scale.testSplit, type = "class")))

# predict results on test data and provide AUC
bad_loan.nn2.pr <- predict(nn2, scale.testSplit, na.rm = TRUE, type = 'raw')
auc_nn2 <- roc(scale.testSplit$loan_quality, bad_loan.nn2.pr)
print(auc_nn2$auc)
```

## Averaging

Since most problems are not linear, the model finds estimates that are only locally optimal (and the parameters have arbitrary initial values), meaning the estimates converge to the closest local minimum. So, we might have very different estimates with similar performance. In order to avoid this issue, several models are started with different values and the results are averaged. This method is called model averaging, and it is used below to optimize predictions. nn3 below uses this method.

```{r whatever, results='markup', cache=TRUE}
# 3nd neural net: averaged neural nets for better optimization
set.seed(1492)
nn3 <- avNNet(loan_quality ~ ., 
              data = scale.trainSplit, 
              size = 3, 
              trace = FALSE,
              entropy = TRUE, # explained in next section
              softmax = TRUE) # explained in next section

# show table
kable(table(scale.testSplit$loan_quality, 
            predict(nn3, scale.testSplit, type = "class")))

# predict results on test data and provide AUC
bad_loan.nn3.pr <- predict(nn3, scale.testSplit, 
                           na.rm = TRUE, type = 'raw')
auc_nn3 <- roc(scale.testSplit$loan_quality, bad_loan.nn3.pr[,2])
print(auc_nn3$auc)
```

## Mathematical Background

In this section, we give the mathematical background to the neural networks we have been running.

In classification problems, there is an issue: the values for f(x) do not sum to 1. So, you take the softmax transformation to solve this and arrive at the probability of each class occurring for that sample. The transformation is as follows:

$$f_{il} = \frac{e^{f_{il}(x)}}{\sum{l}(e^{f_{il}(x)})}$$

Another question you may ask is: what do these neural networks optimize? The answer depends on if the problem is a regression or a classification one. For regression problems, the neural networks minimize the sum of squared errors:

$$\sum_{i=1}^{n} (y_{i} - f_{i}(x))^2$$

The sum of squared errors is minimized to ensure that the predictions are as close as possible to the training data values.

For classification problems, you also have the possibility to maximize the likelihood of a Bernoulli Distribution. The likelihood function is

$$\sum_{l=1}^{C}\sum_{i=1}^{n}y_{ii}*ln(f_{il}^*(x))$$

This function is called entropy. Every observation in the training set has the target variable set to 1 (in case the loan defaults) or 0 (in case it doesn't). This function basically tries to match the distribution in the training set the best it can, and it has more theoretical validity than the SSE approach^[@kuhn:1, 333-334]. In fact, most modern neural networks use this function^[see @deep_lrn and @brownlee for more information]. Neural network functions usually default to the SSE method, but you can change this setting by adding entropy = TRUE in the code (as can be seen in nn5 below).

Finally, how many neurons should we fit in the model? If we fit too few, the model will underfit the data. Conversely, if we fit too many, the model will overfit. It might come down to trial and error, but @heaton:1 indicates a good rule of thumb to start testing: two thirds of the size of the input layer plus the size of the output layer. In our dataset, there are 25 predictors and thus 25 input neurons, and this is a binary classification problem ("good" and "bad") and thus we have 1 output neuron. This would give 17 neurons. In order to save computation time, we will be using only 9 neurons to model our predictions. Also, we could potentially fit more than one hidden layer, but, according to @pacelli:1, in most problems there is no reason for that. Note that the transformations and the change in the hidden layer size improved the ROC for the model.

```{r nnet5, results='markup', cache=TRUE}
# 5th neural net: 9 neurons in hidden layer
# Maximum number of weights increased to 2000 to acommodate 9 hidden units (default is 1000)
set.seed(1492)
nn5 <- avNNet(loan_quality ~ ., 
              data = scale.trainSplit, 
              size = 9, 
              MaxNWts = 2000, 
              softmax = TRUE,   #include softmax transformation 
              trace = FALSE,    #don't output optimization iterations
              entropy = TRUE)   #use entropy instead of least squares

# show table
kable(table(scale.testSplit$loan_quality, 
            predict(nn5, scale.testSplit, type = "class")))

# predict results on test data and provide AUC
bad_loan.nn5.pr <- predict(nn5, scale.testSplit, na.rm = TRUE, type = 'raw')
auc_nn5 <- roc(scale.testSplit$loan_quality, bad_loan.nn5.pr[,2])
print(auc_nn5$auc)
```

## Pre-processing Techniques

Other possibility to increase your model's accuracy is through pre-processing techniques. We have already mentioned scaling (which is a prerequisite to running neural network models), but there are other possibilities, such as centering the data (all values around 0) and the spatial-sign transformation, which projects the data onto the unit circle of the p predictors - so that outliers are brought to the same unit as non-outliers. For this transformation, the data must be centered and scaled.

```{r nnet6, results='markup', cache=TRUE}
# 6th neural net: pre-processing techniques

set.seed(1492)
nnet6 <- train(loan_quality ~ ., data = scale.trainSplit, 
               method = "avNNet", 
               verbose = FALSE,
               trace = FALSE,
               metric = "ROC",
               tuneGrid = data.frame(size = 9, decay = 0, bag = F),
               preProc = c("center", "scale", "spatialSign"),               
               MaxNWts = 2000,
               allowParallel = TRUE,
               softmax = TRUE,
               entropy = TRUE,
               trControl = trainControl(method = "none",
                                        classProbs = TRUE, 
                                        summaryFunction = twoClassSummary))

# show table
kable(table(scale.testSplit$loan_quality, 
            predict(nnet6, scale.testSplit, type = "raw")))

# predict results on test data and provide AUC
bad_loan.nn6.pr <- predict(nnet6, scale.testSplit, na.rm = TRUE, type = 'prob')
auc_nn6 <- roc(scale.testSplit$loan_quality, bad_loan.nn6.pr[,2])
print(auc_nn6$auc)
```

## Decay and Cross Validation

To avoid overfitting, we can use set a weight decay. This weight decay (usually denoted by $\lambda$) penalizes large coefficients on the predictors, so that the predictions become "smoother" and capture less noise. Below is a formula of the sum of squared errors with weight decay:

$$\sum_{i=1}^{n} (y_{i} - f_{i}(x))^2 + \lambda*\sum_{k=1}^{H} \sum_{j=0}^{P}\beta_{jk}^2 + \lambda*\sum_{k=0}^{H} \gamma_{k}^2$$

$\beta_{jk}$ are the coefficients in the initial regression from input to hidden unit and $\gamma_{k}$ are the coefficients from hidden unit to output. So, increasing lambda reduces overfitting. Lambda is a parameter chosen by the user, and it usually ranges between 0 and 2.^[See @kuhn:1 pages 141-145 for more details.]

```{r nnetcomparison, results='markup', cache=TRUE}
# set up CV
cv.9.folds <- createMultiFolds(trainSplit$loan_quality, k = 3, times = 3)
fitControl <- trainControl(method = "repeatedcv", number = 3,
                           repeats = 3, classProbs = TRUE,
                           summaryFunction = twoClassSummary,
                           index = cv.9.folds, search = "grid")

# comparison with weight decays and sizes
nnetGrid <-  expand.grid(size = c(3, 8, 13), decay = c(0,0.2,1,2))
                        
cl <- makeCluster(6, type = "SOCK")
registerDoSNOW(cl)

set.seed(825)
nnet_fit <- train(loan_quality ~ ., data = scale.trainSplit, 
                 method = "nnet", 
                 trControl = fitControl, 
                 verbose = FALSE,
                 trace = FALSE,
                 tuneGrid = nnetGrid,
                 metric = "ROC",
                 allowParallel = TRUE, 
                 MaxNWts = 2000)
stopCluster(cl)
remove(cl)
registerDoSEQ()
```

```{r comparison_graph, cache = TRUE}
nnet_fit
plot(nnet_fit, metric = "ROC")
```

## Best Cutoff Value

Using the cross validation model, let's predict loans using the best cutoff value for nn5 and check the results. Table \@ref(tab:bestcutoffnnet) shows the results, with 67% of bad loans being caught and only 30% of good loans incorrectly considered bad. 

```{r bestcutoffnnet, results='markup', cache=TRUE, tidy=TRUE, tidy.opts=list(width.cutoff = 40)}
# create evaluating test set and subset of test with remaining observations
set.seed(129)
indexes <- sample(nrow(scale.testSplit), 10000)
evaluating_set <- scale.testSplit[indexes,]
test_set_subset <- scale.testSplit[-indexes,]

# run rf1 ROC on evaluating set
roc_predict <- predict(nn5, newdata = evaluating_set, na.rm = TRUE, type = 'prob')
auc <- roc(evaluating_set$loan_quality, roc_predict[,2])

# get "best" cutoff value
nnet_threshold <- coords(auc, x = "best", best.method = "closest.topleft")
nnet_threshold

# predict test_set_subset results with cutoff value
subset_predict <- predict(nn5, newdata = test_set_subset, na.rm = TRUE, type = 'prob')
best_cutoff_predictions <- factor(ifelse(subset_predict[,2] > nnet_threshold[1], "Good", "Bad"))

kable(table(test_set_subset$loan_quality, best_cutoff_predictions),
      caption = 'Confusion Matrix using "best" cutoff threshold',
      label = 'bestcutoffnnet') %>% kable_styling(latex_options=c("hold_position"))
```

# Conclusion

Machine learning can be a powerful tool for many applications, and we have just seen one interesting application in bank loans. We have also seen that there are many approaches to build your model, and each requires a different way to prepare the data. So, don`t forget this: data cleaning is very important! Finally, with this toolset, you are able to start building complex models on your own.

\newpage

# References

<div id="refs"></div>

\newpage

# Appendix A: Predictor Variables Definitions {#datadef}

Below is a breakdown of the explanation for every variable provided in the dataset.

```{r allvars, results='markup', echo=FALSE, cache=TRUE}
all_vars <-data.frame(c("addr_state","annual_inc","annual_inc_joint","application_type",
                        "collection_recovery_fee","collections_12_mths_ex_med","delinq_2yrs","desc","dti","dti_joint","earliest_cr_line",
                        "emp_length","emp_title","fico_range_high","fico_range_low","funded_amnt","funded_amnt_inv","grade","home_ownership",
                        "id","initial_list_status","inq_last_6mths","installment","int_rate","is_inc_v","issue_d","last_credit_pull_d",
                        "last_fico_range_high","last_fico_range_low","last_pymnt_amnt","last_pymnt_d","loan_amnt","loan_status","member_id",
                        "mths_since_last_delinq","mths_since_last_major_derog","mths_since_last_record","next_pymnt_d","open_acc","out_prncp",
                        "out_prncp_inv","policy_code","pub_rec","purpose","pymnt_plan","recoveries","revol_bal","revol_util","sub_grade","term",
                        "title","total_acc","total_pymnt","total_pymnt_inv","total_rec_int","total_rec_late_fee","total_rec_prncp","url",
                        "verified_status_joint","zip_code","open_acc_6m","open_il_6m","open_il_12m","open_il_24m","mths_since_rcnt_il",
                        "total_bal_il","il_util","open_rv_12m","open_rv_24m","max_bal_bc","all_util","total_rev_hi_lim","inq_fi","total_cu_tl",
                        "inq_last_12m","acc_now_delinq","tot_coll_amt","tot_cur_bal"),
                    c("The state provided by the borrower in the loan application","The self-reported annual income provided by the borrower during registration.",
                      "The combined self-reported annual income provided by the co-borrowers during registration",
                      "Indicates whether the loan is an individual application or a joint application with two co-borrowers","post charge off collection fee",
                      "Number of collections in 12 months excluding medical collections",
                      "The number of 30+ days past-due incidences of delinquency in the borrower's credit file for the past 2 years",
                      "Loan description provided by the borrower","A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.",
                      "A ratio calculated using the co-borrowers' total monthly payments on the total debt obligations, excluding mortgages and the requested LC loan, divided by the co-borrowers' combined self-reported monthly income",
                      "The month the borrower's earliest reported credit line was opened",
                      "Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years. ",
                      "The job title supplied by the Borrower when applying for the loan.*",
                      "The upper boundary range the borrower’s FICO at loan origination belongs to.",
                      "The lower boundary range the borrower’s FICO at loan origination belongs to.",
                      "The total amount committed to that loan at that point in time.",
                      "The total amount committed by investors for that loan at that point in time.",
                      "LC assigned loan grade","The home ownership status provided by the borrower during registration. Our values are: RENT, OWN, MORTGAGE, OTHER.",
                      "A unique LC assigned ID for the loan listing.","The initial listing status of the loan. Possible values are – W, F", 
                      "The number of inquiries in past 6 months (excluding auto and mortgage inquiries)",
                      "The monthly payment owed by the borrower if the loan originates.","Interest Rate on the loan",
                      "Indicates if income was verified by LC, not verified, or if the income source was verified","The month which the loan was funded",
                      "The most recent month LC pulled credit for this loan","The upper boundary range the borrower’s last FICO pulled belongs to.",
                      "The lower boundary range the borrower’s last FICO pulled belongs to.","Last total payment amount received","Last month payment was received",
                      "The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.","Current status of the loan","A unique LC assigned Id for the borrower member.",
                      "The number of months since the borrower's last delinquency.","Months since most recent 90-day or worse rating",
                      "The number of months since the last public record.","Next scheduled payment date","The number of open credit lines in the borrower's credit file.","Remaining outstanding principal for total amount funded","Remaining outstanding principal for portion of total amount funded by investors","publicly available policy_code=1, new products not publicly available policy_code=2","Number of derogatory public records",
                      "A category provided by the borrower for the loan request. ","Indicates if a payment plan has been put in place for the loan",
                      "post charge off gross recovery","Total credit revolving balance",
                      "Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.",
                      "LC assigned loan subgrade","The number of payments on the loan. Values are in months and can be either 36 or 60.",
                      "The loan title provided by the borrower","The total number of credit lines currently in the borrower's credit file",
                      "Payments received to date for total amount funded","Payments received to date for portion of total amount funded by investors",
                      "Interest received to date","Late fees received to date","Principal received to date","URL for the LC page with listing data.",
                      "Indicates if the co-borrowers' joint income was verified by LC, not verified, or if the income source was verified",
                      "The first 3 numbers of the zip code provided by the borrower in the loan application.",
                      "Number of open trades in last 6 months","Number of currently active installment trades",
                      "Number of installment accounts opened in past 12 months","Number of installment accounts opened in past 24 months",
                      "Months since most recent installment accounts opened","Total current balance of all installment accounts",
                      "Ratio of total current balance to high credit/credit limit on all install acct","Number of revolving trades opened in past 12 months",
                      "Number of revolving trades opened in past 24 months","Maximum current balance owed on all revolving accounts",
                      "Balance to credit limit on all trades","Total revolving high credit/credit limit","Number of personal finance inquiries",
                      "Number of finance trades","Number of credit inquiries in past 12 months","The number of accounts on which the borrower is now delinquent.",
                      "Total collection amounts ever owed","Total current balance of all accounts"))

# check whether all variables in dataframe are indeed in the loans.csv file
loans_initial <- fread('~/tex/dataexploration/data/loan.csv', sep = ",",header = TRUE)
names(all_vars) <- c("Variable", "Description")
all_vars <- all_vars %>% filter(all_vars$Variable %in% names(loans_initial))

# display table
kable(all_vars, longtable = TRUE)%>% kable_styling(latex_options = c("scale_down")) %>% row_spec(0,bold=TRUE) %>% column_spec(2, width = "4in")
```

# Appendix B: List of Final Predictor Variables {#remaindef}

Below is a breakdown of the model variables.

## Original Variables Description

```{r remained, results='markup', echo=FALSE, cache=TRUE}
# filter remaining original variables definitions
remain_vars <- all_vars[which(all_vars$Variable %in% names(loans)),]
  
#display table
kable(remain_vars, longtable = TRUE)%>% kable_styling(latex_options = c("scale_down")) %>% row_spec(0,bold=TRUE) %>% column_spec(3, width = "4in")
```

## Created Variables Description 

Below is an explanation for every created variable added to the model.
```{r newvars, results='markup', echo=FALSE, cache=TRUE}
# add new variable names to empty vector
new_vars <- vector()

for (i in 1:length(loans)){
  if (!(names(loans)[i] %in% names(loans_initial))){
    new_vars <- c(new_vars, names(loans)[i])
  }
}

new_vars <- as.data.frame(new_vars)
names(new_vars) <- c("Variable")
new_vars$Description <- c("Target variable: displays whether a loan is considered good or bad",
                         "Ratio between borrowers` annual income and total loan amount",
                         "The most recent year LC pulled credit for this loan",
                         "Year loan was taken",
                         "Number of years borrower has had a credit line",
                         "Length of explanation description")

# display table
kable(new_vars, longtable = TRUE)%>% kable_styling(latex_options = c("scale_down")) %>% row_spec(0,bold=TRUE) %>% column_spec(2, width = "4in")
```

# Appendix C: Removed Variables Rationale {#removeddef}

Below is a breakdown of the reason to remove columns. As can be seen, most have been removed because the data is almost homogenous or is unavailable at the time of underwriting:

```{r dropped, results='markup', echo=FALSE, cache=TRUE}
dropped <- data.frame(c("application_type","collection_recovery_fee","collections",
                        "collections_12_mths_ex_med","desc","earliest_cr_line","emp_title",
                        "funded_amnt","funded_amnt_inv","grade","id","int_rate","issue_d",
                        "last_credit_pull_d","last_pymnt_amnt","last_pymnt_d","loan_status",
                        "member_id","next_pymnt_d","out_prncp","out_prncp_inv","policy_code",
                        "pymnt_plan","recoveries","title","total_pymnt","total_pymnt_inv",
                        "total_rec_int","total_rec_late_fee","total_rec_prncp","url",
                        "verified_status_joint","zip_code","tot_coll_amt"),
                      c("254,189 individual applications and 1 joint",
                        "Unknown variable at loan underwriting",
                        "Unknown variable at loan underwriting",
                        "Unknown variable at loan underwriting",
                        "Too convoluted",
                        "Transformed",
                        "Too convoluted",
                        "Unknown variable at loan underwriting",
                        "Unknown variable at loan underwriting",
                        "Dominated by sub_grade",
                        "Randomly assigned to borrower",
                        "Trying to assign int_rate as objective of analysis",
                        "Transformed (too many outcomes to be considered factor)",
                        "Unknown variable at loan underwriting",
                        "Unknown variable at loan underwriting",
                        "Unknown variable at loan underwriting",
                        "Transformed",
                        "Randomly assigned to borrower",
                        "Unknown variable at loan underwriting",
                        "Unknown variable at loan underwriting",
                        "Unknown variable at loan underwriting",
                        "Every observation = 1",
                        "254,188 n and 2 y",
                        "Unknown variable at loan underwriting",
                        "Too convoluted",
                        "Unknown variable at loan underwriting",
                        "Unknown variable at loan underwriting",
                        "Unknown variable at loan underwriting",
                        "Unknown variable at loan underwriting",
                        "Unknown variable at loan underwriting",
                        "Too convoluted",
                        "254,189 N/A and 1 Source Verified",
                        "Too convoluted",
                        "Unknown variable at loan underwriting"
))
names(dropped) <- c("Dropped Variable", "Description")
kable(dropped, longtable = TRUE) %>% row_spec(0,bold=TRUE) %>% kable_styling()
```
